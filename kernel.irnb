{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "d7df94fe-98b8-49de-ace2-bc212adb48ef",
        "_uuid": "8f3d1860b61870b5771f25af81168c9865f5b808",
        "trusted": true
      },
      "cell_type": "code",
      "source": "**Intro**\n\nI decided to take a stab at this project because it looked like a good excercise in unstructured data. Also, the subject really interested me. \n\nThe initial approach I took with this question was analyzing word counts over time, specifically around the time of the crash. The collapse of tulip bulb prices occured in Febuary of 1637, therefore I decided to look at the time period of 1634-1637. This period probably should be extended, but I kept it small to save on time since this is only an exercise.\n\nThe data set is a collection of Dutch newspapers from 1618 to 1699. It was scraped from non-electronic copies using optical character recognition, therefore there are quite a few errors as one could imagine translating 17th century Dutch text into a modern day alphabet.\n\nLet's read in some packages and the CSV file.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "38283c5d-9373-4a2b-822f-9271d064a667",
        "_uuid": "6057e7f9a18792cb1aba691d6b7819c855aaa257",
        "trusted": false
      },
      "cell_type": "code",
      "source": "library(ggplot2, warn.conflicts = FALSE)\nlibrary(reshape2, warn.conflicts = FALSE)\nlibrary(plyr, warn.conflicts = FALSE)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidytext, warn.conflicts = FALSE)\nlibrary(broom, warn.conflicts = FALSE)\n\n\n#Read in CSV\ndutch_newspapers.df <- read.csv(\"../input/dutch_newspapers.csv\")\ndutch_newspapers.df[1,]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0ad9c2bf-3df3-4bee-ac5f-91cf78a3cff7",
        "_uuid": "926f4f02e7928d967f277495b1101dbcc7b45c2c"
      },
      "cell_type": "markdown",
      "source": "I made some changes to the dataframe right off the back. That included adding a \"0\" to all the single digit \"month\" variables. I removed all punctuation in \"text\" and replaced them with \"\", the case could just as easily be made to replace punctuation with \" \", but for now let's start with \"\".  Then, I created a \"date\" variable that combines the \"month\" and \"year\", as a numeric with the year coming first. I did this because I decided on using months as my smallest date increment. If I want to order by date, I can very easily order by this \"date\" variable. "
    },
    {
      "metadata": {
        "collapsed": true,
        "_cell_guid": "94273bae-ed02-4e4d-a9b4-efdf103818a9",
        "_uuid": "f3a4fe114e90ff36148f6c88c346bfecc64100d2",
        "trusted": false
      },
      "cell_type": "code",
      "source": "dutch_newspapers.df <- transform(dutch_newspapers.df,\n                                 month = ifelse((month < 10), paste(0, month, sep = \"\"), month), \n                                 text = gsub(\"[[:punct:]]\", \"\", as.character(text)),\n                                 date = as.numeric(paste(year, ifelse((month < 10), paste(0, month, sep = \"\"), month), sep = \".\")))  ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1364b429-e437-4b53-a33b-9e0ebdc10fcf",
        "_uuid": "c7d81888c98a2f6696fa05c5d768927d59cc20b9"
      },
      "cell_type": "markdown",
      "source": "Using the extremely useful unnest_tokens function in the package TidyText, I selected from the \"text\" columns every word individually. I then removed words that were less than one character in length, I figured that would eliminate a large portion of the dataframe and single letter words are not extremely useful in this analysis."
    },
    {
      "metadata": {
        "_cell_guid": "64b6fd17-e4df-413f-a90f-d7da134d1eea",
        "_uuid": "014805282483fd1df7e86d5c18bea929cf68a904",
        "trusted": false
      },
      "cell_type": "code",
      "source": "dutch_newspapers.df <- unnest_tokens(dutch_newspapers.df[dutch_newspapers.df$date <= 1638.01 & dutch_newspapers.df$date >= 1634.01,], word, text)\n\ndutch_newspapers.df <- dutch_newspapers.df[nchar(dutch_newspapers.df$word) > 2,]\nhead(dutch_newspapers.df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "753bf438-5001-4931-83fc-6cb7489479e0",
        "_uuid": "b5a291f00ef216076f5ce281b967ed2c0c5bea63"
      },
      "cell_type": "markdown",
      "source": "Next, let's reshape the dataframe so that each word has its own column. Then, remove all the words that only appear once. For each month create a column that counts the total amount of words written that month, we will use this to normalize the appearance of words from month to month, since some months may have many more words written than others.  Next, each count was divided by the total number of words counted that month. "
    },
    {
      "metadata": {
        "_cell_guid": "b133d3c2-5c93-49a1-8ee9-2daa5f30c85b",
        "_uuid": "edc19c1da93bcfa38c2f6c75a8264343879d617a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Count the words, dcast the dataframe\nword.count <- dcast(count(dutch_newspapers.df, date, word), date ~ word)\n\n#Remove words that only have 1 character\nword.count <- word.count[,colSums(word.count, na.rm = TRUE) >=2]\n\n#Sum the rows, creatin a total words used for the month\nword.count$sum <- rowSums(word.count[, 2:8459], na.rm = TRUE)\n\nword.count.later <- word.count\nword.count.later[is.na(word.count.later)] <- 0\n\n#Create ratios to normalize the counts\nword.count[, -c(1,8460)] <- word.count[, -c(1,8460)]/word.count[, c(8460)]\n\nhead(word.count[c(1:5,8460)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c165e9cb-fadf-42a5-9120-e7ccd41a976f",
        "_uuid": "f8907935a0aaf882df3125d5715cc3f5af6d3a20"
      },
      "cell_type": "markdown",
      "source": "In order to tie the word counts in with the actual tulip bulb market crash, I decided to use the change in bulb prices and correlate the counts of each words with the bulb prices.  There is not very much information on tulip bulb prices around the time of the crash, the prices I could find I copied into a dataframe. "
    },
    {
      "metadata": {
        "_cell_guid": "7a1a8c69-87ce-407c-940e-36838c8fb4b9",
        "_uuid": "562d0ad0fa5224302a9aa101d293c46cccca1712",
        "trusted": false
      },
      "cell_type": "code",
      "source": "datenum <- c( 6, 22, 34, 36, 37, 38)\ndate <- c( 1634.06, 1635.10, 1636.10, 1636.12, 1637.01, 1637.02)\nprice <- c( 1, 2, 4, 10, 15, 60)\n\nprice.df <- data.frame(datenum, date, price)\n \nplot(datenum, price)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e77415e8-8f40-44cd-a140-2054f9268309",
        "_uuid": "0f2935b3a69777f232d9f16d132959ae5bed61de"
      },
      "cell_type": "markdown",
      "source": "Next, I created two models to infer the missing price points. One is an exponential model fitted to two of the researched data points, the other is a LOESS line fitted to all the researched data points."
    },
    {
      "metadata": {
        "_cell_guid": "245fb236-815f-47bc-8dbd-65d63f2edc0e",
        "_uuid": "820b87d592142ada7016195f1e8f09fc3b14593d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "plot(datenum, price)\n#Two point exponential curve prediction\n\n#Exponential curve created using two points (6,1), (38,60). 1 = a*b^6 and 60 = a*b^38. Solve for a and b.\n# 60/1 = (a*b^38)/(a*b^6) -> 60 = b^32 -> b = 60^(1/32)\n# 1 = a*(60^(1/32))^6 -> a = 1/((60^(1/32))^6)\nb <- 60^(1/32)\na <- 1/((60^(1/32))^6)\n\nlines(datenum, a*(b^datenum), col = \"red\")\n\ntwo.point.function <- function(date){\n  b = 60^(1/32)\n  a = 1/((60^(1/32))^6)\n  price.var = a*(b^date)\n  return(price.var)\n  }\n\nb = 60^(1/32)\na = 1/((60^(1/32))^6)\n\n#Loess prediction\n\nloess.model <- suppressWarnings(loess(price ~ datenum, span = .4))\nlines(datenum, predict(loess.model), col = \"blue\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9b34281b-1183-40d0-97e5-2fa3a12906bf",
        "_uuid": "13cb6e5a4cbdb29392d89f813efc4f07094a7567"
      },
      "cell_type": "markdown",
      "source": "Created a dataframe with the estimated price points for all the dates I am analyzing. I set all the post-crash price points to 0; even though this may not be accurate, there was no information I could find on post-crash prices."
    },
    {
      "metadata": {
        "_cell_guid": "39a1f46a-175c-4f1f-96b0-e00f86f7ad28",
        "_uuid": "a0167cbb69cf3d8bf4e3c7d1e976f7024963b2e3",
        "trusted": false
      },
      "cell_type": "code",
      "source": "v.date <- c()\nfor(i in 1634:1637){\n  for(j in 1:12){\n    g = as.numeric(paste( i, ifelse((j < 10), paste(0, j, sep = \"\"), j), sep = \".\"))\n    v.date <- append(v.date, g)\n  }\n}\n\nprice.df <- as.data.frame(v.date)\nprice.df$datenum <- as.numeric(row.names(price.df))\n\n\nprice.df <- transform(price.df, two.point = ifelse(datenum <= 38, two.point.function(datenum), 0),\n                                loess = ifelse(datenum <= 5, two.point.function(datenum), ifelse(datenum <= 38, predict(loess.model, datenum), 0)))\nhead(price.df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d7f8646b-7b5a-4060-8735-1beb3ecff771",
        "_uuid": "0cb8eda6adfbb78795e1966f2c7dc8684cbaaf9d"
      },
      "cell_type": "markdown",
      "source": "Bind that dataframe with the word.count dataframe."
    },
    {
      "metadata": {
        "_cell_guid": "20df0008-a614-43e3-a5c3-8ccc2b8b1ece",
        "_uuid": "e0adbbcdf3b0a85c2c5e26023fe967d697e2cc35",
        "trusted": false
      },
      "cell_type": "code",
      "source": "count.model.df <- cbind(word.count, price.df[(price.df$v.date %in% word.count$date),])\n\n#Reorder\ncount.model.df <- count.model.df[c(1,8460,8462:8464,2:8459)]\n\n#Change NA's to 0\ncount.model.df[is.na(count.model.df)] <- 0\n\nhead(count.model.df[c(1:8)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "979bcf41-105c-43ec-b522-06f136582e4a",
        "_uuid": "f8d9cbedcbe58588470be55a39d7bde1fc074eca"
      },
      "cell_type": "markdown",
      "source": "Let's run correlation tests for each word with the \"two point\" model price points. Words returned are those with p-values less than 0.05.\n\nLet's take a look at the top four highest correlated words, with at least 15 occurences."
    },
    {
      "metadata": {
        "_cell_guid": "6592e323-3877-4ad3-9d10-e86a69f3e523",
        "_uuid": "d1f333f2552344814797f9639e6bf152b7878ad1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pvalue <- c()\nindex <- c()\nword <- c()\ncor <- c()\nwordcount <- c()\n\nfor(i in 6:8463){\n  test <- cor.test(count.model.df[,4], count.model.df[,i])\n  if(test$p.value <= 0.05){\n    pvalue = append(pvalue, test$p.value)\n    index = append(index, i)\n    word = append(word, colnames(count.model.df[i]))\n    cor = append(cor, test$estimate)\n    wordcount = append(wordcount, sum(word.count.later[[colnames(count.model.df[i])]]))\n  }\n}\n\ntwo.point.df <- data.frame(index, word, pvalue, cor, wordcount)[order(abs(cor), decreasing = TRUE),]\nhead(two.point.df[two.point.df$wordcount >= 15 & abs(two.point.df$cor) >= .55,], n = 4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b3b2384a5faa5f403b1574c2a090e4816da30a50"
      },
      "cell_type": "markdown",
      "source": "Let's see how they look visually."
    },
    {
      "metadata": {
        "_cell_guid": "2a8517a0-17de-470d-8ddf-e06d46977074",
        "_uuid": "896f621854cac1655687f50cb6f19f8cee868a96",
        "trusted": false
      },
      "cell_type": "code",
      "source": "par(mar = c(5,5,2,5),mfrow = c(2, 2))\n#ften\nwith(count.model.df, plot(datenum, ften, col=\"red3\",pch=16, \n                            ylab=expression('\"ften\"')))\npar(new = T)\nwith(count.model.df, plot(datenum, two.point, type = \"l\", lwd = 2, axes=F, xlab=NA, ylab=NA, cex=1.2))\naxis(side = 4)\nmtext(side = 4, line = 3, 'Tulip Bulp Price: TwoPoint')\n\n#opapost\nwith(count.model.df, plot(datenum, opapost, col=\"blue\",pch=16, \n                            ylab=expression('\"opapost\"')))\npar(new = T)\nwith(count.model.df, plot(datenum, two.point, type = \"l\", lwd = 2, axes=F, xlab=NA, ylab=NA, cex=1.2))\naxis(side = 4)\nmtext(side = 4, line = 3, 'Tulip Bulp Price: TwoPoint')\n\n#epnbe\nwith(count.model.df, plot(datenum, epnbe, col=\"orange\", pch=16, \n                          ylab=expression('\"epnbe\"')))\npar(new = T)\nwith(count.model.df, plot(datenum, two.point, type = \"l\", lwd = 2, axes=F, xlab=NA, ylab=NA, cex=1.2))\naxis(side = 4)\nmtext(side = 4, line = 3, 'Tulip Bulp Price: TwoPoint')\n\n#fgne\nwith(count.model.df, plot(datenum, fgne, col=\"green\", pch=16, \n                          ylab=expression('\"fgne\"')))\npar(new = T)\nwith(count.model.df, plot(datenum, two.point, type = \"l\", lwd = 2, axes=F, xlab=NA, ylab=NA, cex=1.2))\naxis(side = 4)\nmtext(side = 4, line = 3, 'Tulip Bulp Price: TwoPoint')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e8b737a4-0e4b-4b2a-97dc-674663666b02",
        "_uuid": "7598992ea87cb84d673adaaa002feb296c5d5351"
      },
      "cell_type": "markdown",
      "source": "Repeat with the LOESS model."
    },
    {
      "metadata": {
        "_cell_guid": "73588a88-72b5-4fc2-88e6-271f6bf2565d",
        "_uuid": "a64c960da181f6127e1ec9de0e31589ab6913d16",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pvalue <- c()\nindex <- c()\nword <- c()\ncor <- c()\nwordcount <- c()\n\nfor(i in 6:8463){\n  test <- cor.test(count.model.df[,5], count.model.df[,i])\n  if(test$p.value <= 0.05){\n    pvalue = append(pvalue, test$p.value)\n    index = append(index, i)\n    word = append(word, colnames(count.model.df[i]))\n    cor = append(cor, test$estimate)\n    wordcount = append(wordcount, sum(word.count.later[[colnames(count.model.df[i])]]))\n  }\n}\n\nloess.df <- data.frame(index, word, pvalue, cor, wordcount)[order(abs(cor), decreasing = TRUE),]\nhead(loess.df[loess.df$wordcount >= 15 & abs(loess.df$cor) >= .55,], n = 4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d8a4ff65-6b48-4d02-a9c7-da62327c5673",
        "_uuid": "221c496d04005be669bdd369b9d9387716be196c"
      },
      "cell_type": "markdown",
      "source": "Here I decided to use a lag function, to see if there were any words that mirrored the price of tulip bulbs but one month in advance, this is a word that could be used as a predictor. First the \"two point\" with lag."
    },
    {
      "metadata": {
        "_cell_guid": "af86143e-640c-4967-b1ec-674d29e800fd",
        "_uuid": "0cfb67834316fca8d0d075865e9022dffbe2dead",
        "trusted": false
      },
      "cell_type": "code",
      "source": "par(mar = c(5,5,2,5),mfrow = c(2, 2))\n\nwith(count.model.df, plot(datenum, opapost, col=\"red3\",pch=16, \n                          ylab=expression('\"opapost\"')))\n\npar(new = T)\nwith(count.model.df, plot(datenum, loess, type = \"l\", lwd = 2, axes=F, xlab=NA, ylab=NA, cex=1.2))\naxis(side = 4)\nmtext(side = 4, line = 3, 'Tulip Bulp Price: Loess')\n\n\n\nwith(count.model.df, plot(datenum, ften, col=\"blue\",pch=16, \n                          ylab=expression('\"ften\"')))\n\npar(new = T)\nwith(count.model.df, plot(datenum, loess, type = \"l\", lwd = 2, axes=F, xlab=NA, ylab=NA, cex=1.2))\naxis(side = 4)\nmtext(side = 4, line = 3, 'Tulip Bulp Price: Loess')\n\n\n\nwith(count.model.df, plot(datenum, croupen, col=\"orange\", pch=16, \n                          ylab=expression('\"croupen\"')))\n\npar(new = T)\nwith(count.model.df, plot(datenum, loess, type = \"l\", lwd = 2, axes=F, xlab=NA, ylab=NA, cex=1.2))\naxis(side = 4)\nmtext(side = 4, line = 3, 'Tulip Bulp Price: Loess')\n\n\nwith(count.model.df, plot(datenum, gun, col=\"green\", pch=16, \n                          ylab=expression('\"gun\"')))\n\npar(new = T)\nwith(count.model.df, plot(datenum, loess, type = \"l\", lwd = 2, axes=F, xlab=NA, ylab=NA, cex=1.2))\naxis(side = 4)\nmtext(side = 4, line = 3, 'Tulip Bulp Price: Loess')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1ded1bd9-d205-4bfb-a0cc-88df4bc0d7f9",
        "_uuid": "42c3e123406adc975a91c08eece73ba1292ed216"
      },
      "cell_type": "markdown",
      "source": "**Summary**\n\nFurther study into the occurences of the words discovered from this analysis would be needed for any significant conclusions. An interesting observation is that the word \"croupen\" is an old word used to describe a respiratory infection. In the  research I did about the market collapse, there were repeated references to an outbreak of the plague that may have lead to the crash. The increasing occurence of the word \"croupen\" may correlate with the increasing worry of the plague outbreak. \n\nOther approaches to this problem would be to use time series analysis of word occurences over many years and see if any words have seasonal trends. If so, did those trends change dramatically around the time of the crash. Another approach is using sentiment analysis to analyze trends in word sentiments. This would require a database of 17th century dutch words with their sentiments attached."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.2",
      "file_extension": ".r",
      "codemirror_mode": "r"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}